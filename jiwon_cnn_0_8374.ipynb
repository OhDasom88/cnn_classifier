{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_0.8374",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1WOUYqg5q1It6EtrLIjsr8Hat0SwxJaVB",
      "authorship_tag": "ABX9TyO9Lj/VCPLrdn6epxXjyMb5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OhDasom88/cnn_classifier/blob/Ryles_CNN/jiwon_cnn_0_8374.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Kq0vLD4gOn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15sl3PE4pPbL",
        "outputId": "3859c72b-c4ff-4862-ee32-db046237e435"
      },
      "source": [
        "!git clone https://github.com/lovit/soynlp.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'soynlp'...\n",
            "remote: Enumerating objects: 5042, done.\u001b[K\n",
            "remote: Counting objects: 100% (585/585), done.\u001b[K\n",
            "remote: Compressing objects: 100% (258/258), done.\u001b[K\n",
            "remote: Total 5042 (delta 374), reused 512 (delta 320), pack-reused 4457\u001b[K\n",
            "Receiving objects: 100% (5042/5042), 34.86 MiB | 28.58 MiB/s, done.\n",
            "Resolving deltas: 100% (3413/3413), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqA87Hee30tK",
        "outputId": "e5a8c72f-de92-4e1f-8050-9486d555e356"
      },
      "source": [
        "!git clone https://github.com/ratsgo/embedding.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'embedding'...\n",
            "remote: Enumerating objects: 1736, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 1736 (delta 73), reused 68 (delta 33), pack-reused 1618\u001b[K\n",
            "Receiving objects: 100% (1736/1736), 448.47 KiB | 10.68 MiB/s, done.\n",
            "Resolving deltas: 100% (1158/1158), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gco1ZANCw-cV",
        "outputId": "2abf050b-d1f6-4494-d1fe-bd34934cd7dd"
      },
      "source": [
        "!git clone https://github.com/lovit/soyspacing.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'soyspacing'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 270 (delta 13), reused 33 (delta 12), pack-reused 235\u001b[K\n",
            "Receiving objects: 100% (270/270), 2.14 MiB | 21.09 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCq6VDqB8eVq",
        "outputId": "cf4174b0-34ac-4d4c-b455-a641506a075c"
      },
      "source": [
        "#영화 데이터 다운로드\n",
        "!wget https://github.com/e9t/nsmc/raw/master/ratings_test.txt\n",
        "!wget https://github.com/e9t/nsmc/raw/master/ratings_train.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-01 13:25:12--  https://github.com/e9t/nsmc/raw/master/ratings_test.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt [following]\n",
            "--2021-08-01 13:25:13--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4893335 (4.7M) [text/plain]\n",
            "Saving to: ‘ratings_test.txt’\n",
            "\n",
            "ratings_test.txt    100%[===================>]   4.67M  28.5MB/s    in 0.2s    \n",
            "\n",
            "2021-08-01 13:25:13 (28.5 MB/s) - ‘ratings_test.txt’ saved [4893335/4893335]\n",
            "\n",
            "--2021-08-01 13:25:13--  https://github.com/e9t/nsmc/raw/master/ratings_train.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt [following]\n",
            "--2021-08-01 13:25:13--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14628807 (14M) [text/plain]\n",
            "Saving to: ‘ratings_train.txt’\n",
            "\n",
            "ratings_train.txt   100%[===================>]  13.95M  43.4MB/s    in 0.3s    \n",
            "\n",
            "2021-08-01 13:25:14 (43.4 MB/s) - ‘ratings_train.txt’ saved [14628807/14628807]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8cDsudz9nBy",
        "outputId": "5ec720e5-93fa-43eb-c0f9-65f2ab0b4033"
      },
      "source": [
        "#konlpy와 mecab 다운로\n",
        "!sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+\n",
        "!sudo apt-get install python-dev; pip install konlpy    \n",
        "!sudo apt-get install curl\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package openjdk-7-jdk is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "E: Package 'openjdk-7-jdk' has no installation candidate\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcurl4 libcurl4-openssl-dev\n",
            "Suggested packages:\n",
            "  libcurl4-doc libidn11-dev libkrb5-dev libldap2-dev librtmp-dev\n",
            "The following packages will be upgraded:\n",
            "  curl libcurl4 libcurl4-openssl-dev\n",
            "3 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
            "Need to get 679 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcurl4-openssl-dev amd64 7.58.0-2ubuntu3.14 [301 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 curl amd64 7.58.0-2ubuntu3.14 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcurl4 amd64 7.58.0-2ubuntu3.14 [219 kB]\n",
            "Fetched 679 kB in 1s (823 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 161122 files and directories currently installed.)\n",
            "Preparing to unpack .../libcurl4-openssl-dev_7.58.0-2ubuntu3.14_amd64.deb ...\n",
            "Unpacking libcurl4-openssl-dev:amd64 (7.58.0-2ubuntu3.14) over (7.58.0-2ubuntu3.13) ...\n",
            "Preparing to unpack .../curl_7.58.0-2ubuntu3.14_amd64.deb ...\n",
            "Unpacking curl (7.58.0-2ubuntu3.14) over (7.58.0-2ubuntu3.13) ...\n",
            "Preparing to unpack .../libcurl4_7.58.0-2ubuntu3.14_amd64.deb ...\n",
            "Unpacking libcurl4:amd64 (7.58.0-2ubuntu3.14) over (7.58.0-2ubuntu3.13) ...\n",
            "Setting up libcurl4:amd64 (7.58.0-2ubuntu3.14) ...\n",
            "Setting up libcurl4-openssl-dev:amd64 (7.58.0-2ubuntu3.14) ...\n",
            "Setting up curl (7.58.0-2ubuntu3.14) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98JzlplsSHp5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "from konlpy.tag import Okt\n",
        "import tqdm\n",
        "import re"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc_VA5GLSHX5"
      },
      "source": [
        "#test, train df 선언\n",
        "test_df=pd.read_csv(\"./ratings_test.txt\", sep=\"\\t\").dropna()\n",
        "train_df=pd.read_csv(\"./ratings_train.txt\", sep=\"\\t\").dropna()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQxZaqN5CP_j"
      },
      "source": [
        "test_path=\"./ratings_test.txt\"\n",
        "train_path=\"./ratings_train.txt\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kPeDE4PSHp5"
      },
      "source": [
        "def load_data_and_label(path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    f = open(path, 'r')\n",
        "    # 첫 줄을 날림.\n",
        "    f.readline()\n",
        "    for line in f.readlines():\n",
        "        data.append(line.split('\\t')[1].replace(\" \", \"\"))\n",
        "        labels.append(line.split('\\t')[2][:-1])\n",
        "    \n",
        "    data = np.asarray(data)\n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwncUswhCXiU"
      },
      "source": [
        "train_sentence, train_labels=load_data_and_label(train_path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Z1mUfwazaW",
        "outputId": "d671a29b-c34a-4e24-a14b-00af8989c750"
      },
      "source": [
        "len(train_labels\n",
        "    )"
      ],
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reSvWsuPxwdr"
      },
      "source": [
        "import soyspacing"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNSoswFfqNtC",
        "outputId": "2e5f30e0-708f-47d3-9c9e-43695de41db9"
      },
      "source": [
        "from soyspacing.soyspacing.countbase import CountSpace\n",
        "\n",
        "corpus_fname=\"/content/ratings_train.txt\"\n",
        "model_fname=\"/content/drive/MyDrive/preprocessed/space-correct.model\"\n",
        "model=CountSpace()\n",
        "model.train(corpus_fname)\n",
        "model.save_model(model_fname, json_format=False)\n",
        "model.load_model(model_fname,json_format=False)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all tags length = 139793 --> 139705, (num_doc = 150000)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg1y7eCZ3o1M"
      },
      "source": [
        "#띄어쓰기 교정, 특수문자 제거\n",
        "\n",
        "def sentence_making(document):\n",
        "  new_doc=[]\n",
        "  for doc in document:\n",
        "   #doc을 \",\"로 스플릿된 리스트로 변환\n",
        "      \n",
        "      doc=doc.split(\" \")\n",
        "      \n",
        "      new_model=[]\n",
        "      for d in doc:\n",
        "        d = re.sub(r\"[^ 가-힣a-zA-Z0-9]\",\" \",d)\n",
        "        new_model.extend(str(model.correct(d)[0]).split(\" \"))\n",
        "      new_model=\" \".join(new_model) \n",
        "      new_doc.append(new_model)\n",
        "  return new_doc\n",
        "  "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHJOrdVuM-Ad"
      },
      "source": [
        "train_cleaned=sentence_making(train_df[\"document\"])\n",
        "test_cleaned=sentence_making(test_df[\"document\"])\n",
        "      "
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWgnWtqyV4jK"
      },
      "source": [
        "new_doc_pd=pd.DataFrame(train_cleaned)\n",
        "new_test_pd=pd.DataFrame(test_cleaned)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EInvQ30Na_qb",
        "outputId": "2069f1e5-0c19-4362-a860-2b234d677be8"
      },
      "source": [
        "len(new_doc_pd)"
      ],
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBV39moeDYcT"
      },
      "source": [
        "new_doc_pd.to_csv(\"/content/drive/MyDrive/preprocessed/new_spacing_doc\", header=0)\n",
        "new_test_pd.to_csv(\"/content/drive/MyDrive/preprocessed/new_spacing_test,\", header=0)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKmzXonaXXLg"
      },
      "source": [
        "new_doc_pd=pd.read_csv(\"/content/drive/MyDrive/preprocessed/new_spacing_doc\",names=[\"id\",\"doc\"])\n",
        "new_test_pd=pd.read_csv(\"/content/drive/MyDrive/preprocessed/new_spacing_test\",names=[\"id\",\"doc\"])\n",
        "\n"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "_gPs9-cdUnxw",
        "outputId": "656ebc52-a535-470f-e973-c02f6c0176ce"
      },
      "source": [
        "new_doc_pd"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>아 더빙 진짜 짜증나네요 목소리</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>흠 포스터보고 초딩영화줄 오버연기조차 가볍지 않구나</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>너무 재밓었다그래서 보는것을 추천한다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>교도소 이야기구먼 솔직히 재미는 없다 평점 조정</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149990</th>\n",
              "      <td>149990</td>\n",
              "      <td>인간이 문제지 소는 뭔죄인가</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149991</th>\n",
              "      <td>149991</td>\n",
              "      <td>평점이 너무 낮아서</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149992</th>\n",
              "      <td>149992</td>\n",
              "      <td>이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149993</th>\n",
              "      <td>149993</td>\n",
              "      <td>청춘 영화의 최고봉 방황과 우울했던 날들의 자화상</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149994</th>\n",
              "      <td>149994</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149995 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                                doc\n",
              "0            0                                  아 더빙 진짜 짜증나네요 목소리\n",
              "1            1                       흠 포스터보고 초딩영화줄 오버연기조차 가볍지 않구나\n",
              "2            2                               너무 재밓었다그래서 보는것을 추천한다\n",
              "3            3                         교도소 이야기구먼 솔직히 재미는 없다 평점 조정\n",
              "4            4  사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 ...\n",
              "...        ...                                                ...\n",
              "149990  149990                                    인간이 문제지 소는 뭔죄인가\n",
              "149991  149991                                         평점이 너무 낮아서\n",
              "149992  149992                      이게 뭐요 한국인은 거들먹거리고 필리핀 혼혈은 착하다\n",
              "149993  149993                        청춘 영화의 최고봉 방황과 우울했던 날들의 자화상\n",
              "149994  149994                           한국 영화 최초로 수간하는 내용이 담긴 영화\n",
              "\n",
              "[149995 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UIttZ9aWcbQ"
      },
      "source": [
        "from konlpy.tag import Mecab, Hannanum, Komoran"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6WDw0JLW7Pi"
      },
      "source": [
        "mecab=Mecab()\n",
        "han=Hannanum()\n",
        "Kom=Komoran()"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nu9wxZR3tyq"
      },
      "source": [
        "#전처리 - 포스태깅 후 명사, 부사, 동사만\n",
        "def preprocessing(text,mecab):\n",
        "  new_text=[]\n",
        "  for word, pos in mecab.pos(text):\n",
        "    \n",
        "    if pos[0] in [\"N\",\"V\",\"M\"] :\n",
        "      \n",
        "        new_text.append(word)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return new_text"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ0SAZQ2JKeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1be9ed-20f2-43fa-a11b-37731daa7373"
      },
      "source": [
        "clean_train_text=[]\n",
        "for text in tqdm.tqdm(new_doc_pd[\"doc\"]):\n",
        "    try:\n",
        "        clean_train_text.append(preprocessing(text, mecab))\n",
        "    except:\n",
        "        clean_train_text.append([])"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 149995/149995 [00:16<00:00, 9045.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZtNdGH6WNt_",
        "outputId": "544a2ca3-c18a-4907-9577-de22c5abef90"
      },
      "source": [
        "clean_test_text=[]\n",
        "for text in tqdm.tqdm(new_test_pd[\"doc\"]):\n",
        "    try:\n",
        "        clean_test_text.append(preprocessing(text, mecab))\n",
        "    except:\n",
        "        clean_test_text.append([])"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49998/49998 [00:05<00:00, 9352.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCPC1giGbx3i",
        "outputId": "634c484b-aebc-4165-d36b-f80bc6fb4c08"
      },
      "source": [
        "import datetime\n",
        "from tensorflow.python.keras.layers.core import Flatten\n",
        "\n",
        "from tensorflow.python.keras.layers.pooling import MaxPool2D\n",
        "start_time = datetime.datetime.now()\n",
        "print(str(start_time))\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "import smart_open   \n",
        "smart_open.open = smart_open.smart_open\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, BatchNormalization\n",
        "from tensorflow.keras import datasets, layers\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import binary_accuracy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# filename = '../movie_review/nsmc/ratings_train.txt'\n",
        "# datastore = pd.read_csv(filename, delimiter='\\t')\n",
        "# filename = '../movie_review/nsmc/ratings_test.txt'\n",
        "# test_data = pd.read_csv(filename, delimiter='\\t')\n",
        "\n",
        "# datastore = pd.read_csv(\"/new_doc\",names=[\"col1\",\"col2\"], header=0)\n",
        "# test_data = pd.read_csv('/new_test',names=[\"col1\",\"col2\"], header=0)\n",
        "\n",
        "\n",
        "# sentences = clean_train_text\n",
        "# labels = train_df[\"label\"]\n",
        "# # ids = datastore.col1.values\n",
        "\n",
        "# test_sentences = clean_test_text\n",
        "# test_labels = test_df[\"label\"]\n",
        "# # test_ids = test_data.col1.values\n",
        "\n",
        "\n",
        "sentences = clean_train_text\n",
        "labels = train_df[\"label\"]\n",
        "# ids = datastore.col1.values\n",
        "\n",
        "test_sentences =clean_test_text\n",
        "test_labels = test_df[\"label\"]\n",
        "# test_ids = test_data.col1.values\n"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-02 01:29:26.104684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW_DGCrPbn9z",
        "outputId": "24185b0e-563c-47ec-b8b8-ae8ef47e972e"
      },
      "source": [
        "len(clean_train_text)"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJH4dz1dY-bk",
        "outputId": "0be17adc-c60a-4020-b476-2fa029452946"
      },
      "source": [
        "type(sentences)"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNcMxhOEbj2V"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<oov>'\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, truncating=trunc_type)\n"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHbbs9TfFQRM"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences_test = tokenizer.texts_to_sequences(test_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=int(np.quantile(np.array([len(seqs) for seqs in sequences]), 0.9)), padding='post', truncating='post')\n",
        "padded_test = pad_sequences(sequences_test, maxlen=padded.shape[-1], padding='post', truncating='post')\n",
        "padded = np.dstack([padded]*2)\n",
        "padded_test = np.dstack([padded_test]*2)\n",
        "\n",
        "vocab_size = len(word_index)\n",
        "embedding_dim = 100\n",
        "max_length = padded.shape[-2]\n",
        "channel_num = 2\n"
      ],
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e6SkoceXUH_j",
        "outputId": "572253dc-9c12-4ada-9ac2-948c17d42aa5"
      },
      "source": [
        "\n",
        "# padded = padded.reshape(len(padded),channel_num,-1)\n",
        "# padded_test = padded_test.reshape(len(padded),channel_num,-1)\n",
        "padded = np.transpose(padded, (0, 2, 1))\n",
        "padded_test = np.transpose(padded_test, (0, 2,1))\n",
        "\n",
        "input1 = Input(shape=(channel_num, max_length))#일단 채널을 전달하고\n",
        "x1 = Embedding(vocab_size+1, embedding_dim)(input1)\n",
        "\n",
        "# x2 = tf.keras.layers.Reshape((max_length, embedding_dim,channel_num))(x1)\n",
        "x2 = tf.transpose(x1, perm=[0,2,3,1])\n",
        "x3 = []#여러개의 filter의 대표값을 저장할 공간\n",
        "\n",
        "for i in range(2,6,1):\n",
        "    #각 그램별 필터는 하나로 계산\n",
        "    x31 = layers.Conv2D(filters=8, kernel_size=(i,embedding_dim), activation='relu', kernel_initializer='glorot_normal')(x2)\n",
        "    x41 = MaxPool2D(pool_size=(x31.shape[1],1))(x31)\n",
        "    x51 = Flatten()(x41)# 각 sample 별로 질문\n",
        "    x3.append(x51)\n",
        "merged = tf.concat(x3, axis=1)\n",
        "x6 = Dense(64, kernel_initializer='glorot_normal')(merged)\n",
        "x = layers.Dropout(0.5)(x6)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=input1, outputs=outputs)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, \n",
        "                                                        len(labels)/batch_size*5, \n",
        "                                                        decay_rate=0.5, \n",
        "                                                        staircase=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_decay)\n",
        "model.compile(optimizer= optimizer, loss= binary_crossentropy, metrics=[binary_accuracy])\n",
        "[]\n",
        "\n",
        "MODEL_SAVE_FOLDER_PATH = '../trained_model'\n",
        "model_file_path = f'{MODEL_SAVE_FOLDER_PATH}/review_cnn-{{epoch:d}}-{{val_loss:.5f}}-{{val_binary_accuracy:.5f}}.hdf5'\n",
        "cb_model_check_point = ModelCheckpoint(filepath=model_file_path, monitor='val_binary_accuracy', verbose=1, save_best_only=True)\n",
        "cb_early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
        "\n",
        "model.fit(padded, np.array(labels).reshape(-1,1), epochs=10, validation_split=0.1, batch_size=32\n",
        ", callbacks=[cb_model_check_point, cb_early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "pred = model.predict(padded_test)\n",
        "print(pred)\n",
        "score = model.evaluate(padded_test,  np.array(test_labels).reshape(-1,1))\n",
        "print(score)\n",
        "# try:\n",
        "#     model = Word2Vec.load(\"./word2vec.model\")\n",
        "# except:\n",
        "#     try:\n",
        "#         with open('./test_corpus.npy', 'rb') as f:\n",
        "#             corpus= np.load(f, allow_pickle=True)\n",
        "#         print(datetime.datetime.now()-start_time)\n",
        "#     except:\n",
        "#         filename = './classifing_movie_review_kr/nsmc/ratings_train.txt'\n",
        "#         data = pd.read_csv(filename, delimiter='\\t')\n",
        "#         print(datetime.datetime.now()-start_time)\n",
        "#         mecab = Mecab(r'C:\\mecab\\mecab-ko-dic')#210625 되는지 확인해보기\n",
        "#         data['document'].fillna('', inplace=True)\n",
        "#         corpus = [mecab.morphs(para) for para in data['document'][:100000]]\n",
        "#         print(datetime.datetime.now()-start_time)\n",
        "\n",
        "#         with open('./test_corpus.npy', 'wb') as f:\n",
        "#             np.save(f, np.array(corpus, dtype=object))\n",
        "#         print(datetime.datetime.now()-start_time)\n",
        "\n",
        "#     model = Word2Vec(sentences=corpus, size=100, window=5, min_count=1, workers=4)\n",
        "#     print(datetime.datetime.now()-start_time)\n",
        "\n",
        "#     model.save(\"./word2vec.model\")\n",
        "\n",
        "# print(model)\n",
        "# # vector = model.wv['금리']  # get numpy vector of a word\n",
        "# # sims = model.wv.most_similar('금리', topn=10)  # get other similar words\n",
        "# # print(vector)\n",
        "# # print(sims)\n"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 2, 21)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 2, 21, 100)   4271400     input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_9 (TFOpL (None, 21, 100, 2)   0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 20, 1, 8)     3208        tf.compat.v1.transpose_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 19, 1, 8)     4808        tf.compat.v1.transpose_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 18, 1, 8)     6408        tf.compat.v1.transpose_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 17, 1, 8)     8008        tf.compat.v1.transpose_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_39 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_36 (Flatten)            (None, 8)            0           max_pooling2d_36[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_37 (Flatten)            (None, 8)            0           max_pooling2d_37[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_38 (Flatten)            (None, 8)            0           max_pooling2d_38[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_39 (Flatten)            (None, 8)            0           max_pooling2d_39[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat_9 (TFOpLambda)        (None, 32)           0           flatten_36[0][0]                 \n",
            "                                                                 flatten_37[0][0]                 \n",
            "                                                                 flatten_38[0][0]                 \n",
            "                                                                 flatten_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 64)           2112        tf.concat_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 64)           0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            65          dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,296,009\n",
            "Trainable params: 4,296,009\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "4219/4219 [==============================] - 131s 30ms/step - loss: 0.4063 - binary_accuracy: 0.8116 - val_loss: 0.3680 - val_binary_accuracy: 0.8374\n",
            "\n",
            "Epoch 00001: val_binary_accuracy improved from -inf to 0.83740, saving model to ../trained_model/review_cnn-1-0.36795-0.83740.hdf5\n",
            "Epoch 2/10\n",
            "4219/4219 [==============================] - 128s 30ms/step - loss: 0.2892 - binary_accuracy: 0.8790 - val_loss: 0.3750 - val_binary_accuracy: 0.8389\n",
            "\n",
            "Epoch 00002: val_binary_accuracy improved from 0.83740 to 0.83887, saving model to ../trained_model/review_cnn-2-0.37497-0.83887.hdf5\n",
            "Epoch 3/10\n",
            "4219/4219 [==============================] - 127s 30ms/step - loss: 0.1745 - binary_accuracy: 0.9320 - val_loss: 0.4594 - val_binary_accuracy: 0.8306\n",
            "\n",
            "Epoch 00003: val_binary_accuracy did not improve from 0.83887\n",
            "Epoch 4/10\n",
            "4219/4219 [==============================] - 126s 30ms/step - loss: 0.1146 - binary_accuracy: 0.9563 - val_loss: 0.5388 - val_binary_accuracy: 0.8194\n",
            "\n",
            "Epoch 00004: val_binary_accuracy did not improve from 0.83887\n",
            "Epoch 5/10\n",
            "4219/4219 [==============================] - 126s 30ms/step - loss: 0.0765 - binary_accuracy: 0.9707 - val_loss: 0.6474 - val_binary_accuracy: 0.8155\n",
            "\n",
            "Epoch 00005: val_binary_accuracy did not improve from 0.83887\n",
            "Epoch 6/10\n",
            "4219/4219 [==============================] - 126s 30ms/step - loss: 0.0592 - binary_accuracy: 0.9767 - val_loss: 0.7403 - val_binary_accuracy: 0.8088\n",
            "\n",
            "Epoch 00006: val_binary_accuracy did not improve from 0.83887\n",
            "Epoch 7/10\n",
            "4219/4219 [==============================] - 126s 30ms/step - loss: 0.0489 - binary_accuracy: 0.9806 - val_loss: 0.8272 - val_binary_accuracy: 0.8098\n",
            "\n",
            "Epoch 00007: val_binary_accuracy did not improve from 0.83887\n",
            "[[4.3785915e-01]\n",
            " [9.6993285e-01]\n",
            " [4.3785915e-01]\n",
            " ...\n",
            " [1.3134949e-02]\n",
            " [1.8798819e-05]\n",
            " [3.9984626e-01]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-355-fe73a5e098d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1628\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1629\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 49998\n  y sizes: 49997\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGrt_9yFOzy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZ5yT79Yf-v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}